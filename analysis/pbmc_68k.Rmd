---
title: "pbmc 68k Experiments"
author: "Eric Weine"
date: "2023-04-07"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## Introduction

Here, I set out to test the run-times of `glmpca` and `fastGLMPCA` on the PBMC 68K dataset prepared by Peter for his fastTopics work. 

## Model Fitting

First, I train the `fastGLMPCA` with only 1-core. 

```{r, eval=FALSE}
library(Matrix)

load("/project2/mstephens/pcarbo/git/fastTopics-experiments/data/pbmc_68k.RData")

#data <- as.matrix(counts)

fit0 <- plash::init_glmpca(
  Y = counts, K = 9, fit_col_size_factor = TRUE, fit_row_intercept = TRUE
)

set.seed(1)

print("data loaded")

library(tictoc)

tic()
fit <- plash::fit_glmpca(Y = counts, fit0 = fit0, algorithm = "ccd", link = "log",
                                    control = list(line_search = TRUE, num_iter = 3), max_iter = 15, warmup = FALSE)
toc()

readr::write_rds(fit, "pbmc_68k_fastGLMPCA_fit_no_warmup_1core.rds")
```

For training the model with 28 cores, I follow the exact same procedure as above.

Now, for glmpca, the fisher optimizer runs into memory issues, because it cannot use sparse matrices without taking stochastic gradients. So, given stochastic gradients must be taken, I decided to use the avagrad optimizer, which is the default.

I fit the model as follows:

```{r, eval=FALSE}
set.seed(1)

library(plash)
library(Matrix)
library(MatrixExtra)
library(tictoc)
library(glmpca)
tic()
fit <- glmpca(Y = counts, L = 9, minibatch = "stochastic", ctl = list(verbose = TRUE, maxIter = 500))
toc()

readr::write_rds(fit, "pbmc_glmpca_sgd_fit_log_long.rds")

```


As a note, `glmpca` does not calculate likelihoods in the original implementation of the package. I added a likelihood calculation into the training, which can be reproduced by installing glmpca via my fork `eweine/glmpca`. 

## Analysis

First, we load in the fitting models that were run on midway.

```{r}
glmpca_sgd <- readr::read_rds(
  "data/pbmc_glmpca_sgd_fit_log_long.rds"
)

fastGLMPCA_1core <- readr::read_rds(
  "data/pbmc_68k_fastGLMPCA_fit_no_warmup_1core.rds"
)

fastGLMPCA_28core <- readr::read_rds(
  "data/pbmc_68k_fastGLMPCA_fit_no_warmup.rds"
)
```

Now, we construct a dataframe to show the progress of each of these algorithms. 

```{r}
loglik_vec <- c()
algo_vec <- c()
time_vec <- c()

loglik_vec <- c(loglik_vec, glmpca_sgd$lik)
algo_vec <- c(algo_vec, rep("glmpca-sgd", length(glmpca_sgd$lik)))
time_vec <- c(time_vec, seq(0, 393, length.out = length(glmpca_sgd$lik)))

loglik_vec <- c(loglik_vec, fastGLMPCA_1core$progress$loglik)
algo_vec <- c(algo_vec, rep("fastGLMPCA-1core", length(fastGLMPCA_1core$progress$loglik)))
time_vec <- c(time_vec, cumsum(fastGLMPCA_1core$progress$time) / 60)

loglik_vec <- c(loglik_vec, fastGLMPCA_28core$progress$loglik)
algo_vec <- c(algo_vec, rep("fastGLMPCA-28core", length(fastGLMPCA_28core$progress$loglik)))
time_vec <- c(time_vec, cumsum(fastGLMPCA_28core$progress$time) / 60)

droplet_time_df <- data.frame(
  loglik = loglik_vec,
  algo = algo_vec,
  time = time_vec
)

library(ggplot2)
library(dplyr)

droplet_time_df <- droplet_time_df %>%
  mutate(dist_from_best = abs(loglik - fastGLMPCA_28core$progress$loglik[11]))

ggplot(data = droplet_time_df) +
  geom_point(aes(x = time, y = dist_from_best, color = algo)) +
  geom_line(aes(x = time, y = dist_from_best, color = algo)) +
  ylab("Distance from Best Log-likelihood") +
  xlab("Time (m)") +
  ggtitle("PBMC 68K Dataset, K = 10")
```

The 28 core version of `fastGLMPCA` converges very quickly. The fisher scoring algorithm of `glmpca` converges second fastest, but to a sub-optimal solution. The reason for this is that `glmpca` increases a penalty when it detects numerical instability in fisher scoring, which results in bias in the final answer. `fastGLMPCA` converges to the optimal solution, though is obvioously much slower than the same algorithm run on 28 scores. Finally, `glmpca` with sgd optimized via avagrad is actually the slowest to converge, though also appears to reach the optimal solution.

